{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJoOrVU2/wXoyy1WDjyI/b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharon-kurant/VCE_Remission_Classification/blob/main/custom_train_VCE_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRvnJImV5Cm2",
        "outputId": "388fb9f6-6375-480c-f84b-95a756f9712a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Piwaar3v4_Co"
      },
      "outputs": [],
      "source": [
        "# ! pip install -q accelerate transformers pytorchvideo evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "YOrXN7Z65HWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, video_dir, num_frames=64, tagged_dir = '/content/drive/MyDrive/remission/data', transform=None, multiplier = 5):\n",
        "        self.video_dir = video_dir\n",
        "        self.num_frames = num_frames\n",
        "        self.transform = transform\n",
        "        self.multiplier = multiplier\n",
        "        # List all video files in the directory\n",
        "        self.video_files = glob.glob(f'{tagged_dir}/**/**/*.mp4') * self.multiplier\n",
        "\n",
        "        class_labels = sorted({str(path).split(\"/\")[-2] for path in self.video_files})\n",
        "        self.label2id = {label: i for i, label in enumerate(class_labels)}\n",
        "        self.id2label = {i: label for label, i in self.label2id.items()}\n",
        "\n",
        "        self.labels = [str(path).split(\"/\")[-2] for path in self.video_files] * self.multiplier\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        video_file = self.video_files[idx]\n",
        "        video_path = os.path.join(self.video_dir, video_file)\n",
        "\n",
        "        # Open the video file\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "        # Get the total number of frames in the video\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        # Randomly select 32 frames\n",
        "        start_frame = torch.randint(0, total_frames - self.num_frames + 1, (1,)).item()\n",
        "        frames = []\n",
        "\n",
        "        # Read frames\n",
        "        for i in range(self.num_frames):\n",
        "            # Set the frame position\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "\n",
        "            # Read the frame\n",
        "            ret, frame = cap.read()\n",
        "\n",
        "            if not ret:\n",
        "                # Handle the case where reading the frame fails\n",
        "                raise RuntimeError(f\"Failed to read frame {i} from video {video_file}\")\n",
        "\n",
        "            # Convert the frame from BGR to RGB\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            if self.transform:\n",
        "              frame = self.transform(frame)\n",
        "            # Append the frame to the list of frames\n",
        "            frames.append(frame)\n",
        "\n",
        "        # Close the video file\n",
        "        cap.release()\n",
        "\n",
        "        # Convert frames to a tensor\n",
        "        frames = torch.stack(frames)\n",
        "\n",
        "        # Apply transformations if provided\n",
        "        # if self.transform:\n",
        "        #     frames = self.transform(frames)\n",
        "\n",
        "        return frames, self.label2id[self.labels[idx]]\n"
      ],
      "metadata": {
        "id": "Je7DOpw25QPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations (if any)\n",
        "transform = transforms.Compose([\n",
        "    # Add any transformations you need (e.g., normalization, resizing)\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.CenterCrop((510,510)),\n",
        "    # transforms.Resize((224, 224)),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Define your dataset and DataLoader\n",
        "video_directory = '/content/drive/MyDrive/remission/data_short_5fps'\n",
        "batch_size = 2  # Adjust as needed\n",
        "multiplier=5\n",
        "# num_classes = 2  # Adjust based on your classification task\n",
        "\n",
        "dataset = VideoDataset(video_directory, num_frames=64, transform=transform, multiplier=multiplier)"
      ],
      "metadata": {
        "id": "YLDrjeXL5wFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n",
        "from transformers import AutoImageProcessor, TimesformerForVideoClassification\n",
        "model_ckpt = \"fcakyon/timesformer-large-finetuned-ssv2\"\n",
        "image_processor = AutoImageProcessor.from_pretrained(model_ckpt)\n",
        "model = TimesformerForVideoClassification.from_pretrained(\n",
        "    model_ckpt,\n",
        "    label2id=dataset.label2id,\n",
        "    id2label=dataset.id2label,\n",
        "    output_hidden_states=True,\n",
        "    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wpxTIA85HbJ",
        "outputId": "5411bfaf-6c16-4a2a-a5c9-2fbac0a85fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at fcakyon/timesformer-large-finetuned-ssv2 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([174, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([174]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_processor = AutoImageProcessor.from_pretrained(model_ckpt)\n",
        "model = TimesformerForVideoClassification.from_pretrained(\n",
        "    model_ckpt,\n",
        "    label2id=dataset.label2id,\n",
        "    id2label=dataset.id2label,\n",
        "    output_hidden_states=True,\n",
        "    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NN0WCRed_2d",
        "outputId": "57137c26-d510-4bbf-fa4c-848be3513186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at fcakyon/timesformer-large-finetuned-ssv2 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([174, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([174]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1\n",
        "\n",
        "# Define your model, loss function, and optimizer\n",
        "# model = VideoClassifierModel()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
        "\n",
        "# Set the number of videos in the dataset (adjust as needed)\n",
        "dataset_videos_amount = 61\n",
        "# Set the number of folds\n",
        "num_folds = 5\n",
        "\n",
        "# Use KFold for 5-fold cross-validation\n",
        "kf = KFold(n_splits=num_folds, shuffle=True, random_state=123)\n",
        "model.cuda()\n",
        "# Training loop\n",
        "for fold, (train_indices, val_indices) in enumerate(kf.split(range(dataset_videos_amount))):\n",
        "    print(f\"Fold {fold + 1}\")\n",
        "\n",
        "    train_examples = np.array([train_index + (r*61) for train_index in train_indices for r in range(multiplier)])\n",
        "    val_examples = np.array([val_index + (r*61) for val_index in val_indices for r in range(multiplier)])\n",
        "    # Split the dataset into training and validation sets\n",
        "    train_set = torch.utils.data.Subset(dataset, train_examples)\n",
        "    val_set = torch.utils.data.Subset(dataset, val_examples)\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Training for the current fold\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # Use tqdm for progress visualization\n",
        "        for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs} - Training'):\n",
        "            frames, labels = batch\n",
        "\n",
        "            # Move data to CUDA\n",
        "            frames, labels = frames.cuda(), labels.cuda()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(frames)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Validation for the current fold\n",
        "        model.eval()\n",
        "        all_labels = []\n",
        "        all_probs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=f'Epoch {epoch + 1}/{num_epochs} - Validation'):\n",
        "                frames, labels = batch\n",
        "\n",
        "                # Move data to CUDA\n",
        "                frames, labels = frames.cuda(), labels.cuda()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(frames)\n",
        "\n",
        "                # Get predicted probabilities\n",
        "                probs = torch.nn.functional.softmax(outputs.logits, dim=1)[:, 1].cpu().numpy()\n",
        "\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_probs.extend(probs)\n",
        "\n",
        "        # Calculate AUC and accuracy\n",
        "        auc = roc_auc_score(all_labels[::multiplier], np.array(all_probs).reshape(-1,multiplier).mean(axis=1))\n",
        "        accuracy = accuracy_score(all_labels[::multiplier], (np.array(all_probs).reshape(-1,multiplier).mean(axis=1) > 0.5).astype(int))\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation AUC: {auc:.4f}, Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcb3qKRi5p4i",
        "outputId": "48a6341f-fbb6-415b-b84a-35fe66b75649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1 - Training: 100%|██████████| 120/120 [05:21<00:00,  2.68s/it]\n",
            "Epoch 1/1 - Validation: 100%|██████████| 33/33 [00:53<00:00,  1.61s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Validation AUC: 0.4167, Accuracy: 0.6923\n",
            "Fold 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1 - Training: 100%|██████████| 123/123 [05:23<00:00,  2.63s/it]\n",
            "Epoch 1/1 - Validation: 100%|██████████| 30/30 [00:44<00:00,  1.50s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Validation AUC: 0.7407, Accuracy: 0.7500\n",
            "Fold 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1 - Training: 100%|██████████| 123/123 [05:16<00:00,  2.58s/it]\n",
            "Epoch 1/1 - Validation: 100%|██████████| 30/30 [00:49<00:00,  1.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Validation AUC: 0.9630, Accuracy: 0.7500\n",
            "Fold 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1 - Training: 100%|██████████| 123/123 [05:09<00:00,  2.52s/it]\n",
            "Epoch 1/1 - Validation: 100%|██████████| 30/30 [00:50<00:00,  1.67s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Validation AUC: 0.6250, Accuracy: 0.6667\n",
            "Fold 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1 - Training: 100%|██████████| 123/123 [04:58<00:00,  2.43s/it]\n",
            "Epoch 1/1 - Validation: 100%|██████████| 30/30 [00:48<00:00,  1.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Validation AUC: 0.6296, Accuracy: 0.7500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v90Lxl7qeaSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_processor = AutoImageProcessor.from_pretrained(model_ckpt)\n",
        "model = TimesformerForVideoClassification.from_pretrained(\n",
        "    model_ckpt,\n",
        "    label2id=dataset.label2id,\n",
        "    id2label=dataset.id2label,\n",
        "    output_hidden_states=True,\n",
        "    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
        ")"
      ],
      "metadata": {
        "id": "S-SUQBK_eaZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 2\n",
        "\n",
        "# Define your model, loss function, and optimizer\n",
        "# model = VideoClassifierModel()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
        "\n",
        "# Set the number of videos in the dataset (adjust as needed)\n",
        "dataset_videos_amount = 61\n",
        "# Set the number of folds\n",
        "num_folds = 5\n",
        "\n",
        "# Use KFold for 5-fold cross-validation\n",
        "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
        "model.cuda()\n",
        "# Training loop\n",
        "for fold, (train_indices, val_indices) in enumerate(kf.split(range(dataset_videos_amount))):\n",
        "    print(f\"Fold {fold + 1}\")\n",
        "\n",
        "    train_examples = np.array([train_index + (r*61) for train_index in train_indices for r in range(multiplier)])\n",
        "    val_examples = np.array([val_index + (r*61) for val_index in val_indices for r in range(multiplier)])\n",
        "    # Split the dataset into training and validation sets\n",
        "    train_set = torch.utils.data.Subset(dataset, train_examples)\n",
        "    val_set = torch.utils.data.Subset(dataset, val_examples)\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Training for the current fold\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # Use tqdm for progress visualization\n",
        "        for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs} - Training'):\n",
        "            frames, labels = batch\n",
        "\n",
        "            # Move data to CUDA\n",
        "            frames, labels = frames.cuda(), labels.cuda()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(frames)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Validation for the current fold\n",
        "        model.eval()\n",
        "        all_labels = []\n",
        "        all_probs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=f'Epoch {epoch + 1}/{num_epochs} - Validation'):\n",
        "                frames, labels = batch\n",
        "\n",
        "                # Move data to CUDA\n",
        "                frames, labels = frames.cuda(), labels.cuda()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(frames)\n",
        "\n",
        "                # Get predicted probabilities\n",
        "                probs = torch.nn.functional.softmax(outputs.logits, dim=1)[:, 1].cpu().numpy()\n",
        "\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_probs.extend(probs)\n",
        "\n",
        "        # Calculate AUC and accuracy\n",
        "        auc = roc_auc_score(all_labels[::multiplier], np.array(all_probs).reshape(-1,multiplier).mean(axis=1))\n",
        "        accuracy = accuracy_score(all_labels[::multiplier], (np.array(all_probs).reshape(-1,multiplier).mean(axis=1) > 0.5).astype(int))\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation AUC: {auc:.4f}, Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "uCdItY8feb9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roc_auc_score(all_labels[::multiplier], np.array(all_probs).reshape(-1,multiplier).mean(axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xpabpq-GCEM",
        "outputId": "7e302715-a815-4926-e172-8b5d15b8bc52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.17500000000000002"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(all_labels[::multiplier], (np.array(all_probs).reshape(-1,multiplier).mean(axis=1) > 0.5).astype(int))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7DS92I9GEpc",
        "outputId": "0ba62625-11df-4278-b082-2cbe40e9842f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6153846153846154"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(all_probs).reshape(-1,multiplier).mean(axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fchFU88mFqOQ",
        "outputId": "952e9627-40f6-40be-d433-ab412c765828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.30135745, 0.30163318, 0.30173445, 0.301951  , 0.30118406,\n",
              "       0.3018083 , 0.30135703, 0.3012586 , 0.3012393 , 0.30138716,\n",
              "       0.3010363 , 0.30114752, 0.30138916], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.ones(26).reshape(-1,multiplier)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "1LgsN7s_V3eH",
        "outputId": "dcba1cd8-fba7-49de-97f9-61e8822dac60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-694a6d1a5bbb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m26\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmultiplier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 26 into shape (2,2)"
          ]
        }
      ]
    }
  ]
}